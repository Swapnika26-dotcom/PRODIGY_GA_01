# PRODIGY_GA_01
Task-01: Text Generation with GPT-2
Objective
The objective of this task is to generate coherent and contextually relevant text using a pre-trained GPT-2 model. The model is fine-tuned on a custom text dataset and used to produce new text based on a user-defined prompt.

Platform
Google Colab / Jupyter Notebook

Model Used
GPT-2 (pre-trained language model)
Technologies Used
Python
Hugging Face Transformers
PyTorch
Datasets library
Dataset
A small custom text dataset was created and used for fine-tuning the GPT-2 model. The dataset consists of simple text sentences related to artificial intelligence and machine learning.

Methodology
Installed required libraries in Google Colab.
Loaded the pre-trained GPT-2 model and tokenizer.
Prepared and tokenized a custom text dataset.
Fine-tuned the GPT-2 model on the dataset.
Generated new text using a user-provided prompt.
Sample Prompt
Task-02: Image Generation using Pre-trained Model
Objective
Generate images from text prompts using a pre-trained Stable Diffusion model.

Platform
Google Colab / Jupyter Notebook

Model Used
Stable Diffusion v1.5

Technologies
Python
Diffusers
PyTorch
Transformers
Sample Prompt
A futuristic city at sunset, digital art

Output
An AI-generated image based on the given text prompt.
Task-03: Markov Chain Text Generator
Description

This task implements a Markov Chainâ€“based text generator using Google Colab.
The program accepts user input text and generates new text based on probabilistic word transitions.

Technologies Used

Python

Google Colab

Features

User input through interactive text box

Button-based text generation

Markov chain logic

Example

Input:

machine learning generates text


Output:

learning generates text

Conclusion

Task-03 demonstrates a simple and effective use of Markov Chains for text generation, suitable for academic assignments.
